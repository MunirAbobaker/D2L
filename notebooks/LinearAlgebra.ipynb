{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3c9c19",
   "metadata": {},
   "source": [
    "### Scalar\n",
    "- Scalares are just one numeric values\n",
    "- lower-cased letter as notation e.x. x, y, z\n",
    "-  all continuous real-valued Scalars R\n",
    "-  $x \\in  R$ , means: x is a real-values scalar. The symbole means \"in\"\n",
    "-  $x, y \\in  {0,1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a9a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://int.repositories.cloud.sap/artifactory/api/pypi/build-releases-pypi/simple, https://int.repositories.cloud.sap/artifactory/api/pypi/build-milestones-pypi/simple\n",
      "Requirement already satisfied: torch in c:\\users\\d073999\\miniconda3\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\d073999\\miniconda3\\lib\\site-packages (from torch) (4.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\d073999\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\d073999\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\d073999\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\d073999\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\d073999\\miniconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415766f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed017313",
   "metadata": {},
   "source": [
    "### Vecotrs\n",
    "- list of scalar values.\n",
    "- named elements, entries or components of the vector.\n",
    "- e.x. loan defaults prediction represented as vector with income, length of employment and number of previous defaults as its components.\n",
    "- vectors denoted as bold-faced, lower-cased letters (e.g., :math:`\\mathbf{x}`,\n",
    ":math:`\\mathbf{y}`, and :math:`\\mathbf{z})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9012884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b3bfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51d0b1",
   "metadata": {},
   "source": [
    "### Length, Dimensionality, and shape\n",
    "- length of vector is commonly called dimension of a vector\n",
    "- access lenght by len function in python \n",
    "- access lenght by len function in python or shape in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cbc9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede49bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812cfb8",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "- vectors generlize scalars from order zero to order one, matrices generlize vectors from order one to order two.\n",
    "- denoted with bald-faced, capital letters \n",
    "- represented in code as tensors with two axes.\n",
    "- m rows and n columns of real-valued scalars.\n",
    "- shape is (m x n) \n",
    "- matrix with same number of rows and columns is squared matrix\n",
    "- A *transpose* means  exchage rows and columns.\n",
    "- if B = A *transpose*, tehn bij = aji for any i and j\n",
    "- A: is symetric matrix if it is sqaure matrix and A = *A transpose*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12735e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4); A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcb87f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57de2508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2,0,4], [3,4,5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb40998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e6f95",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "- here: refers to algebraic objects\n",
    "- n-dimensional arrays with arbitrary number of axes.\n",
    "- vectors are first order, matrces and tensors are second order\n",
    "-  denoted with capital lettters of special font face\n",
    "-  indexing mechanism is similar to that of metrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b66091c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2,3,4); X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62be46",
   "metadata": {},
   "source": [
    "### Basic Properties of Tensor Arithmetic\n",
    "- any elementwise unary operation does not change the shape of its operand.\n",
    "- binary elementwise operation on any two tensors of the same shape, will result a tensor of that same shape e.x., add two matrecies\n",
    "- Hadamard Product is elementwise multiplication of two metrecies, denoted as circle with point in the center.\n",
    "\n",
    "- Mulitply or add a scalar will not change the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea90a51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone() # Assign a copy of 'A' to 'B' by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf24130a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B # Hadamard Product: [a11b11 a12b12 .... a1nb1n \n",
    "                          \n",
    "                                         #]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "348fad06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2 \n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cbaab",
   "metadata": {},
   "source": [
    "### Reduction\n",
    "- some of elements of a tensor\n",
    "- symbole is :math:`\\sum`\n",
    "- can also be used to sum element of matrx of size (m, n)\n",
    "- by default, sum reduces a tensor along all its axes to a scalar\n",
    "- one can specifiy the axes along which the tensor is reduced via summation.\n",
    "- dimension is lost. e.x, (3,2), sum along axes = 0, results in a tensor with a vector of shape 2 (3 is lost, axes = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "178c55d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum() # reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8655de05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4957116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of A (5, 4)\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape # axes =0 , which is 5 is lost, we have vector of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd211304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis=1, 4 is lost, result vector of shape 5\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1ad80",
   "metadata": {},
   "source": [
    "### None-Reduction Sum\n",
    "- keeps number axes non changed e.i., array of shape (3,2): sum it a long the axis=1, but keep it 3 rows and 1 columen(sum of the 2 other columns). With the normal reduction u will get a vector with a size of 3. Dimension is not lost\n",
    "-  we can use broadcasting to divide the original method and the new one (after we used non-reduction sum)\n",
    "-  cumsum will not change the axis. If one performed cumsum along zero-axis, each row is the sum of the previous rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1409d4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39059256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd757215",
   "metadata": {},
   "source": [
    "### 2.3.7 Dot Products\n",
    "- sum of the elementwise multiplications\n",
    "-  if we have a vector x and we multiply it with a wieght vector w. if the weights are not negative and sum to one, the dot product express a weighted average.\n",
    "-  After normlizing the two  vectors to have a unit length, the dot product express the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aedbb19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "361b961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y) # same as above sum of element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426eb59f",
   "metadata": {},
   "source": [
    "### 2.3.8. Matrix-Vector Products\n",
    "- is a column vector\n",
    "- we can represent rotations as multiplications by a square matrix.\n",
    "- use matrix-vector mulitplication to describe the most intensive calculations \n",
    " required when computing each layer in a neural network given the values of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15ec5fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5a89a",
   "metadata": {},
   "source": [
    "### 2.3.9 Matrix-Matrix Multiplication\n",
    "- think of it as performing m matrix-vector products and stitching the results together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b90e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616c8bd",
   "metadata": {},
   "source": [
    "### 2.3.10 Norms\n",
    "-  informally, norm of a vector tells us how big the vector is (size not dimensionality, rather the magnitude of the components).\n",
    "-  it is a function that maps a vector to a scaler, satisfying a handful of properties:\n",
    "\t\t- if we scale all elements of a vector by a constant factor, its norm also scales by the absolute value of the same constant factor.\n",
    "\t\t- traingle inequality: f(x+y) <= f(x) + f(y)\n",
    "\t\t- norm must be non-negative (least size is zero)\n",
    "\t\t- smallest norm (zero) is only achieved by a vector cosisting of all zeros.\n",
    "\t\t- Euclidian distance is the L2 norm\n",
    "\t\t- L1 norm is the sum of the absolute values of the vector elements.\n",
    "\t\t- L1 is less influnced by outliers.\n",
    "\t\t- L1 & L2 are casses of Lp norm\n",
    "\t\t-  Analogous to L2 norms of vectors, the Frobenius norm of a matrix is the sum of the sqaures of the matrix elments\n",
    "\t\t-  The forbenius norm satisfy the property of a vector norm. It is like L2 norm for matrix-shaped vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de8b0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u) # L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a951752",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(u).sum() # L1 norm some of the absolute values of the vector elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c19a32a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9))) # Forbenius norm, as L2 norm for Matrix-shaped vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1275d",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Scalars, vectors, matrices, and tensors are basic mathematical objects in linear algebra.\n",
    "\n",
    "Vectors generalize scalars, and matrices generalize vectors.\n",
    "\n",
    "Scalars, vectors, matrices, and tensors have zero, one, two, and an arbitrary number of axes, respectively.\n",
    "\n",
    "A tensor can be reduced along the specified axes by sum and mean.\n",
    "\n",
    "Elementwise multiplication of two matrices is called their Hadamard product. It is different from matrix multiplication.\n",
    "\n",
    "In deep learning, we often work with norms such as the  norm, the  norm, and the Frobenius norm.\n",
    "\n",
    "We can perform a variety of operations over scalars, vectors, matrices, and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6868507e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c74874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "print(A == (A.T).T) # transpose of A-transpose is A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e10d48cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True],\n",
      "        [True, True, True, True, True],\n",
      "        [True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# given two matrices A and B, proof that the sum of the transposes is the same as the transpose of the sum\n",
    "print((A.T + B.T) == (A + B).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6c36cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X) # X is a tensor and the output will be the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77340a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = torch.arange(50).reshape(5, 5, 2)\n",
    "len(X1) # X is a tensor and the output will be the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a75e47fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A / A.sum(axis=1) \n",
    "# error, cuz we summed the  along axis 1, si we endup with a vector of \n",
    "#size 5(nr. of rows) and we try to divde matrix with 4 column with a vector of 5 elments\n",
    "# if we use this A/ A.sum(axis= 0) or we only have a scaler (singelton), it will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06010dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:2 torch.Size([5, 4])\n",
      "tensor([40., 45., 50., 55.])\n"
     ]
    }
   ],
   "source": [
    "#analysis\n",
    "print( \"shape:2\", A.shape)\n",
    "print(A.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2282d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38., 54., 70.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea987c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 when travelling between two points in Manhatten, what is the distance that you \n",
    "# would cover in term of the coordiante, i.e., in terms of avenues and streets? can you travel diagonally?\n",
    "# one can go up, down, right, left, but no diagonal directions are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a24dd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = torch.arange(24, dtype=torch.float32).reshape(2,3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e1d2997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.7571)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf9913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
